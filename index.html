<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Avner May</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Avner May</h1>
</div>
<table class="imgtable"><tr><td>
<img src="pictures/Avner_May_Picture_Together.jpg" alt="Avner May" width="190px" height="190px" />&nbsp;</td>
<td align="left"><p>Staff Research Scientist at <a href="https://together.ai">Together.ai</a><br />
<a href="files/Avner_May_Resume_2023.pdf">CV</a></p>
</td></tr></table>
<h2>Contact Information<br /></h2>
<p>Email: avnermay [at] &lt;google's email service&gt; (dot) com <br /></p>
<h2>About me</h2>
<p>I am a staff research scientist at <a href="https://together.ai/">together.ai</a>. Prior to joining Together, I was a research scientist in Google's speech recognition group (2020-2023), and a postdoctoral scholar working in <a href="https://cs.stanford.edu/people/chrismre/">Prof. Chris Ré's</a> group at Stanford University (2018-2020). I completed my PhD in Computer Science at Columbia University in December 2017, advised by <a href="http://www.cs.columbia.edu/~mcollins/">Prof. Michael Collins</a>. Prior to my PhD, I worked for two years as a software development engineer at Microsoft, living in Seattle, WA.  I graduated in 2009 from Harvard College, where I majored in Mathematics, with a minor in Computer Science. I am originally from Potomac, MD.</p>
<h2>Research Interests</h2>
<p>My research interests center around designing simpler, better understood, and more efficient, machine learning models. For example, during my PhD, I showed that kernel approximation methods can perform comparably to fully-connected deep neural networks on the challenging non-linear classification problems in speech recognition systems. More recently, I have worked on better understanding what makes an approximate feature representation perform well on downstream tasks, both in the context of kernel approximation methods and word embedding compression. This understanding is important for efficiently selecting among existing feature approximations or designing new ones, and for navigating the trade-offs between computation, memory, and downstream performance.
</p>
<p>Prior to working on machine learning, I did two years of research in social network analysis, advised by <a href="http://www.cs.columbia.edu/~augustin/">Prof. Augustin Chaintreau</a>; I studied whether social networks like Facebook or Twitter are efficient systems for delivering content of interest to their users (2011-2013).</p>
<h2>Other Interests</h2>
<p>I love most things that involve being active and outdoors&#8201;&mdash;&#8201;running, biking, snowboarding, hiking, camping, and basically anything in the mountains.  During the summer of 2017 I spent 2.5 months on the Pacific Crest Trail.  I am very interested in food systems and nutrition, and how they affect our health, the environment, and the well-being of animals.</p>
<h2>Publications <br /></h2>
<ul>
<li><p><a href="files/ACL_2020_contextual_embeddings.pdf"><b>Contextual Embeddings: When are they worth it?</b></a> <br />
S. Arora*, <b>A. May*</b>, J. Zhang, C. Ré <br />
<i>ACL 2020</i></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2003.04983.pdf"><b>Understanding the Downstream Instability of Word Embeddings</b></a> <br />
M. Leszczynski, <b>A. May</b>, J. Zhang, S. Wu, C. Aberger, C. Ré <br />
<i>MLSys 2020</i></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1909.01264.pdf"><b>On the Downstream Performance of Compressed Word Embeddings</b></a> <br />
<b>A. May</b>, J. Zhang, T. Dao, C. Ré <br />
<i>NeurIPS 2019 (Spotlight, 3% acceptance)</i> <a href="files/compressed_embeddings_neurips_2019_slides.pdf">[slides]</a> <a href="files/compressed_embeddings_neurips_2019_video.mp4">[video]</a></p>
</li>
<li><p><a href="http://proceedings.mlr.press/v89/zhang19f/zhang19f.pdf"><b>Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation</b></a> <br />
J. Zhang*, <b>A. May*</b>, T. Dao, C. Ré <br />
<i>AISTATS 2019</i></p>
</li>
<li><p><a href="http://www.jmlr.org/papers/volume20/17-026/17-026.pdf"><b>Kernel Approximation Methods for Speech Recognition</b></a> <br />
<b>A. May</b>, A.B. Garakani, Z. Lu, D. Guo, K. Liu, A. Bellet, L. Fan, M. Collins, D. Hsu, B. Kingsbury, M. Picheny, F. Sha <br />
<i>JMLR 2019 (arXiv 2017)</i></p>
</li>
<li><p><a href="files/may_thesis.pdf"><b>Kernel Approximation Methods for Speech Recognition</b></a> <br />
<b>Avner May</b> <br />
<i>PhD Thesis, 2017</i> <a href="files/thesis_defense_slides.pdf">[slides]</a></p>
</li>
<li><p><a href="files/ICASSP_2016_Compact_Kernels.pdf"><b>Compact Kernel Models for Acoustic Modeling via Random Feature Selection</b></a> <br />
<b>A. May</b>, M. Collins, D. Hsu, B. Kingsbury <br />
<i>ICASSP 2016</i> <br /></p>
</li>
<li><p><a href="files/ICASSP_2016_Comparison_DNNs_Kernels.pdf"><b>A Comparison Between Deep Neural Nets and Kernel Acoustic Models for Speech Recognition</b></a> <br />
Z. Lu, D. Guo, A.B. Garakani, K. Liu, <b>A. May</b>, A. Bellet, L. Fan, M. Collins, B. Kingsbury, M. Picheny, F. Sha <br />
<i>ICASSP 2016</i>


</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1411.4000v1.pdf"><b>How to Scale Up Kernel Methods to Be As Good As Deep Neural Nets</b></a> <br />
Z. Lu*, <b>A. May*</b>, K. Lu, A. Garakani, D. Guo, A. Bellet, L. Fan, F. Sha, M. Collins, B. Kingsbury <br />
<i>arXiv 2014</i> <br /></p>
</li>
<li><p><a href="files/Sigmetrics2014_Filter_and_Follow.pdf"><b>Filter &amp; Follow: How Social Media Foster Content Curation</b></a> <br />
<b>A. May</b>, A. Chaintreau, N. Korula, S. Lattanzi <br />
<i>SIGMETRICS 2014</i></p>
</li>
</ul>
<p>* Equal contribution.</p>
<h2>Internships</h2>
<ul>
<li><p>Summer 2015: Google Research, New York, NY.</p>
</li>
<li><p>Summer 2014: Microsoft Research, Redmond, WA.</p>
</li>
</ul>
<h2>Community Service</h2>
<ul>
<li><p>I have been a reviewer for ICLR 2018, ICML 2017-2020 (2019 Top Reviewer), NeurIPS 2017-2019, ICJAI 2019-2020 (2019 Distinguished PC member), AAAI 2020. </p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2023-11-02 00:51:03 UTC, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</div>
</body>
</html>
